{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "This cell imports essential libraries required for data processing, visualization, and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "This cell loads the training and testing datasets using `pandas.read_csv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying First Few Rows\n",
    "To get an overview of the dataset, this cell displays the first few rows using `train_data.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First few rows of the dataset:\")\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Dataset Information\n",
    "This cell prints the dataset's structure, including column names, data types, and non-null values using `train_data.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "This cell identifies numerical and categorical columns, imputes missing numerical values with the median, and replaces missing categorical values with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Missing Data\n",
    "# Check if 'Response' column exists\n",
    "if \"Response\" not in train_data.columns:\n",
    "    print(\"Error: 'Response' column is missing from train_data! Check your dataset.\")\n",
    "\n",
    "else:\n",
    "    # Identify numerical and categorical columns (excluding 'Response' from numerical)\n",
    "    numerical_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.drop(\"Response\", errors=\"ignore\")\n",
    "    categorical_cols = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Impute missing numerical values with median\n",
    "    for col in numerical_cols:\n",
    "        median_value = train_data[col].median()\n",
    "        train_data[col].fillna(median_value, inplace=True)\n",
    "        test_data[col].fillna(median_value, inplace=True)  # Use train's median for consistency\n",
    "\n",
    "    # Impute missing categorical values with mode\n",
    "    for col in categorical_cols:\n",
    "        mode_value = train_data[col].mode()[0]\n",
    "        train_data[col].fillna(mode_value, inplace=True)\n",
    "        test_data[col].fillna(mode_value, inplace=True)\n",
    "\n",
    "    print(\"Missing values handled successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Scaling Data\n",
    "Applies label encoding to categorical features and standard scaling to numerical features to prepare them for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "categorical_cols = [\"Accomodation_Type\", \"Reco_Insurance_Type\"]\n",
    "label_enc = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    train_data[col] = label_enc.fit_transform(train_data[col])\n",
    "    test_data[col] = label_enc.transform(test_data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'ID' column if exists\n",
    "if 'ID' in train_data.columns:\n",
    "    train_data.drop(\"ID\", axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"No 'ID' column found in train_data.\")\n",
    "\n",
    "if 'ID' in test_data.columns:\n",
    "    test_IDs = test_data[\"ID\"]\n",
    "    test_data.drop(\"ID\", axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"No 'ID' column found in test_data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features & target variable\n",
    "X = train_data.drop(\"Response\", axis=1)\n",
    "y = train_data[\"Response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data\n",
    "This cell splits the dataset into training and testing sets using `train_test_split()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train & validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all features are numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_val = X_val.apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data\n",
    "This cell identifies numerical and categorical columns, imputes missing numerical values with the median, and replaces missing categorical values with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle NaN values after conversion\n",
    "X_train.fillna(-1, inplace=True)\n",
    "X_val.fillna(-1, inplace=True)\n",
    "test_data.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and Scaling Data\n",
    "This cell applies label encoding to categorical features and standard scaling to numerical features to prepare them for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Machine Learning Models\n",
    "Below cells trains multiple machine learning models including Random Forest, Gradient Boosting, and others to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(n_estimators=200, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression model\n",
    "log_reg_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "log_reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM model\n",
    "svm_model = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each model\n",
    "y_val_pred_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_gb = gb_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_log_reg = log_reg_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_svm = svm_model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to binary (threshold: 0.5)\n",
    "y_val_pred_rf_binary = (y_val_pred_rf >= 0.5).astype(int)\n",
    "y_val_pred_gb_binary = (y_val_pred_gb >= 0.5).astype(int)\n",
    "y_val_pred_xgb_binary = (y_val_pred_xgb >= 0.5).astype(int)\n",
    "y_val_pred_log_reg_binary = (y_val_pred_log_reg >= 0.5).astype(int)\n",
    "y_val_pred_svm_binary = (y_val_pred_svm >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Graph\n",
    "\n",
    "The feature importance graph is a visualization of the importance of each feature in the model. It is a useful tool for understanding which features are most influential in the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using Random Forest\n",
    "plt.figure(figsize=(10, 5))\n",
    "feature_importance = rf_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)[-10:]  # Top 10 Features\n",
    "plt.barh(X.columns[sorted_idx], feature_importance[sorted_idx], color=\"teal\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features (Random Forest)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using XGBoost\n",
    "plt.figure(figsize=(10, 5))\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "sorted_xgb_idx = np.argsort(xgb_importance)[-10:]\n",
    "plt.barh(X.columns[sorted_xgb_idx], xgb_importance[sorted_xgb_idx], color=\"purple\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using Gradient Boosting\n",
    "plt.figure(figsize=(10, 5))\n",
    "gb_importance = gb_model.feature_importances_\n",
    "sorted_gb_idx = np.argsort(gb_importance)[-10:]\n",
    "plt.barh(X.columns[sorted_gb_idx], gb_importance[sorted_gb_idx], color=\"orange\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features (Gradient Boosting)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using Logistic Regression (absolute coefficients)\n",
    "plt.figure(figsize=(10, 5))\n",
    "log_reg_importance = np.abs(log_reg_model.coef_).flatten()\n",
    "sorted_log_reg_idx = np.argsort(log_reg_importance)[-10:]\n",
    "plt.barh(X.columns[sorted_log_reg_idx], log_reg_importance[sorted_log_reg_idx], color=\"blue\")\n",
    "plt.xlabel(\"Coefficient Value (Absolute)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features (Logistic Regression)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance using SVM (absolute coefficients)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Extract feature importance\n",
    "svm_importance = np.abs(svm_model.coef_).flatten()\n",
    "sorted_svm_idx = np.argsort(svm_importance)[-10:]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(X.columns[sorted_svm_idx], svm_importance[sorted_svm_idx], color=\"red\")\n",
    "plt.xlabel(\"Coefficient Value (Absolute)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Top 10 Important Features (SVM - Linear Kernel)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "This cell calculates the AUC-ROC score to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC-AUC for each model\n",
    "roc_auc_rf = roc_auc_score(y_val, y_val_pred_rf)\n",
    "roc_auc_gb = roc_auc_score(y_val, y_val_pred_gb)\n",
    "roc_auc_xgb = roc_auc_score(y_val, y_val_pred_xgb)\n",
    "roc_auc_log_reg = roc_auc_score(y_val, y_val_pred_log_reg)\n",
    "roc_auc_svm = roc_auc_score(y_val, y_val_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ROC-AUC scores\n",
    "print(f\"ROC-AUC score for Random Forest: {roc_auc_rf:.4f}\")\n",
    "print(f\"ROC-AUC score for Gradient Boosting: {roc_auc_gb:.4f}\")\n",
    "print(f\"ROC-AUC score for XGBoost: {roc_auc_xgb:.4f}\")\n",
    "print(f\"ROC-AUC score for Logistic Regression: {roc_auc_log_reg:.4f}\")\n",
    "print(f\"ROC-AUC score for SVM: {roc_auc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models dictionary BEFORE using models.keys()\n",
    "models = {\n",
    "    \"Random Forest\": y_val_pred_rf_binary,\n",
    "    \"Gradient Boosting\": y_val_pred_gb_binary,\n",
    "    \"XGBoost\": y_val_pred_xgb_binary,\n",
    "    \"Logistic Regression\": y_val_pred_log_reg_binary,\n",
    "    \"SVM Model\": y_val_pred_svm_binary\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of the best model\n",
    "The best model is selected based on ROC-AUC score. The model with the highest ROC-AUC score is considered the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on ROC-AUC score\n",
    "roc_auc_scores = {\n",
    "    \"Random Forest\": roc_auc_rf,\n",
    "    \"Gradient Boosting\": roc_auc_gb,\n",
    "    \"XGBoost\": roc_auc_xgb,\n",
    "    \"Logistic Regression\": roc_auc_log_reg,\n",
    "    \"SVM\": roc_auc_svm\n",
    "}\n",
    "\n",
    "best_model_name = max(roc_auc_scores, key=roc_auc_scores.get)\n",
    "print(f\"Best model selected: {best_model_name}\")\n",
    "\n",
    "if best_model_name == \"Random Forest\":\n",
    "    best_model = rf_model\n",
    "elif best_model_name == \"Gradient Boosting\":\n",
    "    best_model = gb_model\n",
    "elif best_model_name == \"XGBoost\":\n",
    "    best_model = xgb_model\n",
    "elif best_model_name == \"Logistic Regression\":\n",
    "    best_model = log_reg_model\n",
    "else:\n",
    "    best_model = svm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curves for all models\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define model predictions and labels\n",
    "model_preds = {\n",
    "    \"Random Forest\": y_val_pred_rf,\n",
    "    \"Gradient Boosting\": y_val_pred_gb,\n",
    "    \"XGBoost\": y_val_pred_xgb,\n",
    "    \"Logistic Regression\": y_val_pred_log_reg,\n",
    "    \"SVM\": y_val_pred_svm\n",
    "}\n",
    "\n",
    "# Plot ROC Curves\n",
    "for model_name, y_pred in model_preds.items():\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc_scores[model_name]:.4f})\")\n",
    "\n",
    "# Plot diagonal reference line\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot AUC Scores as a Bar Chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(roc_auc_scores.keys(), roc_auc_scores.values(), color=[\"blue\", \"green\", \"red\", \"purple\"])\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"AUC Score\")\n",
    "plt.title(\"AUC Score Comparison Across Models\")\n",
    "plt.ylim(0.5, 1)  # Ensures a consistent scale\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Display AUC values on bars\n",
    "for i, v in enumerate(roc_auc_scores.values()):\n",
    "    plt.text(i, v + 0.01, f\"{v:.4f}\", ha=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect classification report data\n",
    "metrics = [\"precision\", \"recall\", \"f1-score\"]\n",
    "model_names = list(models.keys())\n",
    "\n",
    "# Initialize a dictionary to store scores\n",
    "scores = {metric: [] for metric in metrics}\n",
    "\n",
    "# Extract scores from classification reports\n",
    "for model_name, y_pred in models.items():\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    for metric in metrics:\n",
    "        scores[metric].append(report[\"weighted avg\"][metric])\n",
    "\n",
    "# Plot bar chart\n",
    "x = np.arange(len(model_names))  # the label locations\n",
    "width = 0.2  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i * width, scores[metric], width, label=metric)\n",
    "\n",
    "# Formatting the plot\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Comparison of Classification Metrics Across Models\")\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(model_names, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "for model_name, y_pred in models.items():\n",
    "    print(f\"\\nClassification Report for {model_name}:\\n\")\n",
    "    report = classification_report(y_val, y_pred, output_dict=True)\n",
    "    df_report = pd.DataFrame(report).transpose()\n",
    "    print(df_report)\n",
    "\n",
    "    # Plot classification report heatmap\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(df_report.iloc[:-1, :-1], annot=True, cmap=\"Blues\", fmt=\".2f\")\n",
    "    plt.title(f'Classification Report for {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final prediction on the test data\n",
    "y_test_pred = best_model.predict_proba(test_data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert response to binary (threshold: 0.5)\n",
    "y_test_pred_binary = (y_test_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save submission files\n",
    "os.makedirs(\"submission_folder\", exist_ok=True)\n",
    "\n",
    "submission = pd.DataFrame({\"ID\": test_IDs, \"Response\": y_test_pred})\n",
    "submission.to_csv(\"submission_folder/submission.csv\", index=False)\n",
    "\n",
    "submission_binary = pd.DataFrame({\"ID\": test_IDs, \"Response\": y_test_pred_binary})\n",
    "submission_binary.to_csv(\"submission_folder/submission_binary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file with both submissions\n",
    "zipf = zipfile.ZipFile(\"submission.zip\", \"w\", zipfile.ZIP_DEFLATED)\n",
    "for file in [\"submission.csv\", \"submission_binary.csv\"]:\n",
    "    zipf.write(f\"submission_folder/{file}\", file)\n",
    "zipf.close()\n",
    "\n",
    "print(\"Submission zip file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
